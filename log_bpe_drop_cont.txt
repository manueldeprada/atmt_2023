INFO: Commencing training!
INFO: COMMAND: integrated_train.py --log-file=log_bpe_drop_cont.txt
INFO: Arguments: {'cuda': True, 'data': 'data/en-fr/raw', 'dicts': 'data/en-fr/sp_all', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 4, 'train_on_tiny': False, 'arch': 'lstm', 'bpe': True, 'bpe_dropout': 0.1, 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 5, 'save_dir': 'assignments/03/bpe_dropout', 'restore_file': 'checkpoint_best.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False}
INFO: Loaded a source dictionary (fr) with 8000 words
INFO: Loaded a target dictionary (en) with 8000 words
INFO: Built a model with 2336576 parameters
INFO: Loaded checkpoint assignments/03/bpe_dropout/checkpoint_best.pt
INFO: Epoch 090: loss 4.213 | lr 0.0003 | num_tokens 12.95 | batch_size 4 | grad_norm 50.15 | clip 1
INFO: Epoch 090: valid_loss 4.37 | num_tokens 13.3 | batch_size 500 | valid_perplexity 78.8
INFO: Epoch 091: loss 3.75 | lr 0.0003 | num_tokens 12.97 | batch_size 4 | grad_norm 40.17 | clip 1
INFO: Epoch 091: valid_loss 4.09 | num_tokens 13.1 | batch_size 500 | valid_perplexity 59.7
INFO: Epoch 092: loss 3.547 | lr 0.0003 | num_tokens 12.96 | batch_size 4 | grad_norm 37.21 | clip 1
INFO: Epoch 092: valid_loss 3.92 | num_tokens 13 | batch_size 500 | valid_perplexity 50.2
INFO: Epoch 093: loss 3.441 | lr 0.0003 | num_tokens 12.93 | batch_size 4 | grad_norm 35.77 | clip 1
INFO: Epoch 093: valid_loss 3.82 | num_tokens 13.1 | batch_size 500 | valid_perplexity 45.5
INFO: Epoch 094: loss 3.378 | lr 0.0003 | num_tokens 12.94 | batch_size 4 | grad_norm 34.86 | clip 1
INFO: Epoch 094: valid_loss 3.81 | num_tokens 13.1 | batch_size 500 | valid_perplexity 45.2
INFO: Epoch 095: loss 3.3 | lr 0.0003 | num_tokens 12.95 | batch_size 4 | grad_norm 34.57 | clip 1
INFO: Epoch 095: valid_loss 3.73 | num_tokens 13 | batch_size 500 | valid_perplexity 41.9
INFO: Epoch 096: loss 3.242 | lr 0.0003 | num_tokens 12.94 | batch_size 4 | grad_norm 33.81 | clip 1
INFO: Epoch 096: valid_loss 3.69 | num_tokens 13 | batch_size 500 | valid_perplexity 40.1
INFO: Epoch 097: loss 3.206 | lr 0.0003 | num_tokens 12.93 | batch_size 4 | grad_norm 33.65 | clip 1
INFO: Epoch 097: valid_loss 3.63 | num_tokens 13 | batch_size 500 | valid_perplexity 37.8
INFO: Epoch 098: loss 3.189 | lr 0.0003 | num_tokens 12.93 | batch_size 4 | grad_norm 33.16 | clip 1
INFO: Epoch 098: valid_loss 3.64 | num_tokens 13 | batch_size 500 | valid_perplexity 38.3
INFO: Epoch 099: loss 3.145 | lr 0.0003 | num_tokens 12.92 | batch_size 4 | grad_norm 32.87 | clip 1
INFO: Epoch 099: valid_loss 3.61 | num_tokens 13 | batch_size 500 | valid_perplexity 37
INFO: Epoch 100: loss 3.122 | lr 0.0003 | num_tokens 12.96 | batch_size 4 | grad_norm 32.81 | clip 1
INFO: Epoch 100: valid_loss 3.56 | num_tokens 13 | batch_size 500 | valid_perplexity 35
INFO: Epoch 101: loss 3.106 | lr 0.0003 | num_tokens 12.97 | batch_size 4 | grad_norm 32.54 | clip 1
INFO: Epoch 101: valid_loss 3.52 | num_tokens 13.1 | batch_size 500 | valid_perplexity 33.8
INFO: Epoch 102: loss 3.083 | lr 0.0003 | num_tokens 12.94 | batch_size 4 | grad_norm 32.38 | clip 1
INFO: Epoch 102: valid_loss 3.55 | num_tokens 13.1 | batch_size 500 | valid_perplexity 34.9
INFO: Epoch 103: loss 3.06 | lr 0.0003 | num_tokens 12.97 | batch_size 4 | grad_norm 32.41 | clip 1
INFO: Epoch 103: valid_loss 3.52 | num_tokens 12.9 | batch_size 500 | valid_perplexity 33.8
INFO: Epoch 104: loss 3.027 | lr 0.0003 | num_tokens 12.92 | batch_size 4 | grad_norm 32.28 | clip 1
INFO: Epoch 104: valid_loss 3.5 | num_tokens 12.9 | batch_size 500 | valid_perplexity 33.3
INFO: Epoch 105: loss 3.018 | lr 0.0003 | num_tokens 12.94 | batch_size 4 | grad_norm 32.23 | clip 1
INFO: Epoch 105: valid_loss 3.49 | num_tokens 13.2 | batch_size 500 | valid_perplexity 32.9
INFO: Epoch 106: loss 2.991 | lr 0.0003 | num_tokens 12.91 | batch_size 4 | grad_norm 32.17 | clip 1
INFO: Epoch 106: valid_loss 3.46 | num_tokens 13.1 | batch_size 500 | valid_perplexity 31.9
INFO: Epoch 107: loss 2.98 | lr 0.0003 | num_tokens 12.93 | batch_size 4 | grad_norm 32.11 | clip 1
INFO: Epoch 107: valid_loss 3.47 | num_tokens 13 | batch_size 500 | valid_perplexity 32.1
INFO: Epoch 108: loss 2.976 | lr 0.0003 | num_tokens 12.95 | batch_size 4 | grad_norm 32.12 | clip 1
INFO: Epoch 108: valid_loss 3.41 | num_tokens 13.1 | batch_size 500 | valid_perplexity 30.2
INFO: Epoch 109: loss 2.954 | lr 0.0003 | num_tokens 12.91 | batch_size 4 | grad_norm 32.08 | clip 1
INFO: Epoch 109: valid_loss 3.43 | num_tokens 13.3 | batch_size 500 | valid_perplexity 31
INFO: Epoch 110: loss 2.944 | lr 0.0003 | num_tokens 12.93 | batch_size 4 | grad_norm 32.1 | clip 1
INFO: Epoch 110: valid_loss 3.39 | num_tokens 13 | batch_size 500 | valid_perplexity 29.6
INFO: Epoch 111: loss 2.929 | lr 0.0003 | num_tokens 12.94 | batch_size 4 | grad_norm 31.96 | clip 1
INFO: Epoch 111: valid_loss 3.42 | num_tokens 13.2 | batch_size 500 | valid_perplexity 30.5
INFO: Epoch 112: loss 2.92 | lr 0.0003 | num_tokens 12.93 | batch_size 4 | grad_norm 31.91 | clip 1
INFO: Epoch 112: valid_loss 3.35 | num_tokens 13.1 | batch_size 500 | valid_perplexity 28.4
INFO: Epoch 113: loss 2.893 | lr 0.0003 | num_tokens 12.92 | batch_size 4 | grad_norm 31.68 | clip 1
INFO: Epoch 113: valid_loss 3.37 | num_tokens 13 | batch_size 500 | valid_perplexity 29
INFO: Epoch 114: loss 2.883 | lr 0.0003 | num_tokens 12.95 | batch_size 4 | grad_norm 31.6 | clip 1
INFO: Epoch 114: valid_loss 3.35 | num_tokens 13.1 | batch_size 500 | valid_perplexity 28.6
INFO: Epoch 115: loss 2.887 | lr 0.0003 | num_tokens 12.94 | batch_size 4 | grad_norm 31.72 | clip 1
INFO: Epoch 115: valid_loss 3.32 | num_tokens 13.1 | batch_size 500 | valid_perplexity 27.7
INFO: Epoch 116: loss 2.875 | lr 0.0003 | num_tokens 12.92 | batch_size 4 | grad_norm 31.57 | clip 1
INFO: Epoch 116: valid_loss 3.3 | num_tokens 12.9 | batch_size 500 | valid_perplexity 27.2
INFO: Epoch 117: loss 2.872 | lr 0.0003 | num_tokens 12.97 | batch_size 4 | grad_norm 31.78 | clip 1
INFO: Epoch 117: valid_loss 3.32 | num_tokens 13.2 | batch_size 500 | valid_perplexity 27.8
INFO: Epoch 118: loss 2.858 | lr 0.0003 | num_tokens 12.96 | batch_size 4 | grad_norm 31.5 | clip 1
INFO: Epoch 118: valid_loss 3.31 | num_tokens 13 | batch_size 500 | valid_perplexity 27.4
INFO: Epoch 119: loss 2.834 | lr 0.0003 | num_tokens 12.93 | batch_size 4 | grad_norm 31.54 | clip 1
INFO: Epoch 119: valid_loss 3.28 | num_tokens 12.9 | batch_size 500 | valid_perplexity 26.5
INFO: Epoch 120: loss 2.835 | lr 0.0003 | num_tokens 12.93 | batch_size 4 | grad_norm 31.83 | clip 1
INFO: Epoch 120: valid_loss 3.29 | num_tokens 13.1 | batch_size 500 | valid_perplexity 26.8
INFO: Epoch 121: loss 2.824 | lr 0.0003 | num_tokens 12.96 | batch_size 4 | grad_norm 31.67 | clip 1
INFO: Epoch 121: valid_loss 3.31 | num_tokens 13.1 | batch_size 500 | valid_perplexity 27.4
INFO: Epoch 122: loss 2.817 | lr 0.0003 | num_tokens 12.94 | batch_size 4 | grad_norm 31.47 | clip 1
INFO: Epoch 122: valid_loss 3.31 | num_tokens 12.9 | batch_size 500 | valid_perplexity 27.4
INFO: Epoch 123: loss 2.811 | lr 0.0003 | num_tokens 12.98 | batch_size 4 | grad_norm 31.47 | clip 1
INFO: Epoch 123: valid_loss 3.24 | num_tokens 13 | batch_size 500 | valid_perplexity 25.6
INFO: Epoch 124: loss 2.801 | lr 0.0003 | num_tokens 12.92 | batch_size 4 | grad_norm 31.26 | clip 1
INFO: Epoch 124: valid_loss 3.28 | num_tokens 13.1 | batch_size 500 | valid_perplexity 26.5
INFO: Epoch 125: loss 2.796 | lr 0.0003 | num_tokens 12.92 | batch_size 4 | grad_norm 31.51 | clip 1
INFO: Epoch 125: valid_loss 3.22 | num_tokens 12.9 | batch_size 500 | valid_perplexity 25.1
INFO: Epoch 126: loss 2.781 | lr 0.0003 | num_tokens 12.92 | batch_size 4 | grad_norm 31.28 | clip 1
INFO: Epoch 126: valid_loss 3.26 | num_tokens 13.2 | batch_size 500 | valid_perplexity 25.9
INFO: Epoch 127: loss 2.772 | lr 0.0003 | num_tokens 12.95 | batch_size 4 | grad_norm 31.26 | clip 1
INFO: Epoch 127: valid_loss 3.25 | num_tokens 13.1 | batch_size 500 | valid_perplexity 25.8
INFO: Epoch 128: loss 2.775 | lr 0.0003 | num_tokens 12.95 | batch_size 4 | grad_norm 31.35 | clip 1
INFO: Epoch 128: valid_loss 3.25 | num_tokens 13.1 | batch_size 500 | valid_perplexity 25.7
INFO: Epoch 129: loss 2.764 | lr 0.0003 | num_tokens 12.95 | batch_size 4 | grad_norm 31.34 | clip 1
INFO: Epoch 129: valid_loss 3.24 | num_tokens 13.1 | batch_size 500 | valid_perplexity 25.5
INFO: Epoch 130: loss 2.764 | lr 0.0003 | num_tokens 12.94 | batch_size 4 | grad_norm 31.24 | clip 1
INFO: Epoch 130: valid_loss 3.2 | num_tokens 13.1 | batch_size 500 | valid_perplexity 24.6
INFO: Epoch 131: loss 2.74 | lr 0.0003 | num_tokens 12.9 | batch_size 4 | grad_norm 31.45 | clip 1
INFO: Epoch 131: valid_loss 3.2 | num_tokens 13 | batch_size 500 | valid_perplexity 24.6
INFO: Epoch 132: loss 2.732 | lr 0.0003 | num_tokens 12.94 | batch_size 4 | grad_norm 31.16 | clip 1
INFO: Epoch 132: valid_loss 3.18 | num_tokens 13.1 | batch_size 500 | valid_perplexity 24.1
INFO: Epoch 133: loss 2.728 | lr 0.0003 | num_tokens 12.91 | batch_size 4 | grad_norm 31.29 | clip 1
INFO: Epoch 133: valid_loss 3.2 | num_tokens 12.9 | batch_size 500 | valid_perplexity 24.5
INFO: Epoch 134: loss 2.727 | lr 0.0003 | num_tokens 12.93 | batch_size 4 | grad_norm 31.36 | clip 1
INFO: Epoch 134: valid_loss 3.18 | num_tokens 13 | batch_size 500 | valid_perplexity 24.1
INFO: Epoch 135: loss 2.74 | lr 0.0003 | num_tokens 12.97 | batch_size 4 | grad_norm 31.28 | clip 1
INFO: Epoch 135: valid_loss 3.17 | num_tokens 13.1 | batch_size 500 | valid_perplexity 23.9
INFO: Epoch 136: loss 2.719 | lr 0.0003 | num_tokens 12.94 | batch_size 4 | grad_norm 31.26 | clip 1
INFO: Epoch 136: valid_loss 3.19 | num_tokens 13 | batch_size 500 | valid_perplexity 24.3
INFO: Epoch 137: loss 2.716 | lr 0.0003 | num_tokens 12.96 | batch_size 4 | grad_norm 31.32 | clip 1
INFO: Epoch 137: valid_loss 3.14 | num_tokens 13 | batch_size 500 | valid_perplexity 23.1
INFO: Epoch 138: loss 2.699 | lr 0.0003 | num_tokens 12.92 | batch_size 4 | grad_norm 31.22 | clip 1
INFO: Epoch 138: valid_loss 3.19 | num_tokens 12.9 | batch_size 500 | valid_perplexity 24.3
INFO: Epoch 139: loss 2.684 | lr 0.0003 | num_tokens 12.9 | batch_size 4 | grad_norm 31.15 | clip 1
INFO: Epoch 139: valid_loss 3.15 | num_tokens 13.1 | batch_size 500 | valid_perplexity 23.2
INFO: Epoch 140: loss 2.684 | lr 0.0003 | num_tokens 12.9 | batch_size 4 | grad_norm 31.21 | clip 1
INFO: Epoch 140: valid_loss 3.17 | num_tokens 13.1 | batch_size 500 | valid_perplexity 23.8
INFO: Epoch 141: loss 2.696 | lr 0.0003 | num_tokens 12.94 | batch_size 4 | grad_norm 31.04 | clip 1
INFO: Epoch 141: valid_loss 3.11 | num_tokens 13 | batch_size 500 | valid_perplexity 22.5
INFO: Epoch 142: loss 2.674 | lr 0.0003 | num_tokens 12.94 | batch_size 4 | grad_norm 31.17 | clip 1
INFO: Epoch 142: valid_loss 3.13 | num_tokens 13.2 | batch_size 500 | valid_perplexity 22.8
INFO: Epoch 143: loss 2.674 | lr 0.0003 | num_tokens 12.96 | batch_size 4 | grad_norm 31.39 | clip 1
INFO: Epoch 143: valid_loss 3.14 | num_tokens 13.3 | batch_size 500 | valid_perplexity 23.1
INFO: Epoch 144: loss 2.671 | lr 0.0003 | num_tokens 12.92 | batch_size 4 | grad_norm 31.17 | clip 1
INFO: Epoch 144: valid_loss 3.08 | num_tokens 13.2 | batch_size 500 | valid_perplexity 21.7
INFO: Epoch 145: loss 2.662 | lr 0.0003 | num_tokens 12.93 | batch_size 4 | grad_norm 31.13 | clip 1
INFO: Epoch 145: valid_loss 3.12 | num_tokens 13.1 | batch_size 500 | valid_perplexity 22.8
INFO: Epoch 146: loss 2.66 | lr 0.0003 | num_tokens 12.93 | batch_size 4 | grad_norm 31.16 | clip 1
INFO: Epoch 146: valid_loss 3.12 | num_tokens 13.1 | batch_size 500 | valid_perplexity 22.6
INFO: Epoch 147: loss 2.659 | lr 0.0003 | num_tokens 12.95 | batch_size 4 | grad_norm 31.2 | clip 1
INFO: Epoch 147: valid_loss 3.1 | num_tokens 13 | batch_size 500 | valid_perplexity 22.2
INFO: Epoch 148: loss 2.635 | lr 0.0003 | num_tokens 12.96 | batch_size 4 | grad_norm 31.12 | clip 1
INFO: Epoch 148: valid_loss 3.12 | num_tokens 13.1 | batch_size 500 | valid_perplexity 22.7
INFO: Epoch 149: loss 2.641 | lr 0.0003 | num_tokens 12.94 | batch_size 4 | grad_norm 31.16 | clip 1
INFO: Epoch 149: valid_loss 3.11 | num_tokens 13 | batch_size 500 | valid_perplexity 22.4
INFO: No validation set improvements observed for 5 epochs. Early stop!
